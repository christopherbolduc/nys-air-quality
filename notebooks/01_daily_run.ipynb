{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily AQ notebook — configuration\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load .env from repo root\n",
    "env_file = Path(\"..\") / \".env\"\n",
    "for line in env_file.read_text(encoding=\"utf-8\").splitlines():\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "        continue\n",
    "    k, v = line.split(\"=\", 1)\n",
    "    os.environ.setdefault(k.strip(), v.strip().strip('\"').strip(\"'\"))\n",
    "\n",
    "OPENAQ_API_KEY = os.environ[\"OPENAQ_API_KEY\"]\n",
    "\n",
    "# BBOX is a performance hint only; NY membership is enforced via the NY GeoJSON boundary.\n",
    "BBOX = os.getenv(\"BBOX\", \"-79.8,40.4,-71.6,45.1\")\n",
    "SAMPLE_SIZE = int(os.getenv(\"SAMPLE_SIZE\", \"100\"))\n",
    "STALE_HOURS = int(os.getenv(\"STALE_HOURS\", \"12\"))\n",
    "\n",
    "TODAY_UTC = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"TODAY_UTC:\", TODAY_UTC)\n",
    "print(\"SAMPLE_SIZE:\", SAMPLE_SIZE)\n",
    "print(\"STALE_HOURS:\", STALE_HOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull a location catalog for the region. This is used to build the daily sample.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "OPENAQ_BASE_URL = \"https://api.openaq.org/v3\"\n",
    "\n",
    "t0 = time.time()\n",
    "resp = requests.get(\n",
    "    f\"{OPENAQ_BASE_URL}/locations\",\n",
    "    params={\n",
    "        \"bbox\": BBOX,\n",
    "        \"iso\": \"US\",\n",
    "        \"limit\": 1000,\n",
    "    },\n",
    "    headers={\"X-API-Key\": OPENAQ_API_KEY},\n",
    "    timeout=30,\n",
    ")\n",
    "latency_ms = int((time.time() - t0) * 1000)\n",
    "\n",
    "resp.raise_for_status()\n",
    "locations_payload = resp.json()\n",
    "\n",
    "locations_found = locations_payload.get(\"meta\", {}).get(\"found\")\n",
    "locations = locations_payload.get(\"results\", [])\n",
    "\n",
    "print(\"status:\", resp.status_code)\n",
    "print(\"latency_ms:\", latency_ms)\n",
    "print(\"locations_found:\", locations_found)\n",
    "print(\"locations_returned:\", len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16638e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NY state boundary (GeoJSON Feature)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "boundary_path = Path(\"..\") / \"data\" / \"nys_boundary.geojson\"\n",
    "ny_feature = json.loads(boundary_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "geom = ny_feature.get(\"geometry\") or {}\n",
    "geom_type = geom.get(\"type\")\n",
    "coords = geom.get(\"coordinates\")\n",
    "\n",
    "if geom_type not in {\"Polygon\", \"MultiPolygon\"}:\n",
    "    raise ValueError(f\"Unsupported geometry type: {geom_type}\")\n",
    "\n",
    "print(\"geometry:\", geom_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter locations to points inside the NY boundary polygon.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "def point_in_ring(lon: float, lat: float, ring: list[list[float]]) -> bool:\n",
    "    # Ray casting algorithm. Ring is a list of [lon, lat] points.\n",
    "    inside = False\n",
    "    j = len(ring) - 1\n",
    "    for i in range(len(ring)):\n",
    "        xi, yi = ring[i]\n",
    "        xj, yj = ring[j]\n",
    "        intersects = ((yi > lat) != (yj > lat)) and (\n",
    "            lon < (xj - xi) * (lat - yi) / ((yj - yi) or 1e-15) + xi\n",
    "        )\n",
    "        if intersects:\n",
    "            inside = not inside\n",
    "        j = i\n",
    "    return inside\n",
    "\n",
    "def point_in_polygon(lon: float, lat: float, polygon: list[list[list[float]]]) -> bool:\n",
    "    # polygon: [outer_ring, hole1, hole2, ...]\n",
    "    if not polygon:\n",
    "        return False\n",
    "    outer = polygon[0]\n",
    "    if not point_in_ring(lon, lat, outer):\n",
    "        return False\n",
    "    # If inside a hole, treat as outside.\n",
    "    for hole in polygon[1:]:\n",
    "        if point_in_ring(lon, lat, hole):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def point_in_multipolygon(lon: float, lat: float, multipolygon: list) -> bool:\n",
    "    for polygon in multipolygon:\n",
    "        if point_in_polygon(lon, lat, polygon):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def loc_lon_lat(loc: dict) -> tuple[float | None, float | None]:\n",
    "    c = loc.get(\"coordinates\") or {}\n",
    "    try:\n",
    "        return float(c.get(\"longitude\")), float(c.get(\"latitude\"))\n",
    "    except (TypeError, ValueError):\n",
    "        return None, None\n",
    "\n",
    "geom = ny_feature[\"geometry\"]\n",
    "geom_type = geom[\"type\"]\n",
    "coords = geom[\"coordinates\"]\n",
    "\n",
    "ny_locations: list[dict] = []\n",
    "missing_coords = 0\n",
    "\n",
    "for loc in locations:\n",
    "    lon, lat = loc_lon_lat(loc)\n",
    "    if lon is None or lat is None:\n",
    "        missing_coords += 1\n",
    "        continue\n",
    "\n",
    "    inside = (\n",
    "        point_in_polygon(lon, lat, coords) if geom_type == \"Polygon\"\n",
    "        else point_in_multipolygon(lon, lat, coords)\n",
    "    )\n",
    "    if inside:\n",
    "        ny_locations.append(loc)\n",
    "\n",
    "print(\"locations_total:\", len(locations))\n",
    "print(\"missing_coords:\", missing_coords)\n",
    "print(\"ny_locations:\", len(ny_locations))\n",
    "print(\"first_5_names:\", [l.get(\"name\") for l in ny_locations[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f159a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable daily sample (deterministic ordering) from NY-only locations.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def stable_rank(location: dict) -> int:\n",
    "    raw = (str(location.get(\"id\", \"\")) + \"|\" + BBOX).encode(\"utf-8\")\n",
    "    return int(hashlib.sha256(raw).hexdigest(), 16)\n",
    "\n",
    "ny_sorted = sorted(ny_locations, key=stable_rank)\n",
    "sampled_locations = ny_sorted[: min(SAMPLE_SIZE, len(ny_sorted))]\n",
    "\n",
    "sample_ids = [loc[\"id\"] for loc in sampled_locations if \"id\" in loc]\n",
    "sample_names = [loc.get(\"name\") for loc in sampled_locations[:5]]\n",
    "\n",
    "print(\"ny_locations:\", len(ny_locations))\n",
    "print(\"sample_size:\", len(sample_ids))\n",
    "print(\"first_5_names:\", sample_names)\n",
    "print(\"first_5_ids:\", sample_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch latest measurements for the sampled locations.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def fetch_latest_for_location(location_id: int) -> dict:\n",
    "    url = f\"{OPENAQ_BASE_URL}/locations/{location_id}/latest\"\n",
    "    r = requests.get(url, headers={\"X-API-Key\": OPENAQ_API_KEY}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "latest_results: list[dict] = []\n",
    "errors: list[tuple[int, str]] = []\n",
    "\n",
    "for loc_id in sample_ids:\n",
    "    try:\n",
    "        latest_results.append(fetch_latest_for_location(loc_id))\n",
    "    except Exception as e:\n",
    "        errors.append((loc_id, str(e)))\n",
    "\n",
    "elapsed_s = time.time() - t0\n",
    "\n",
    "print(\"locations_requested:\", len(sample_ids))\n",
    "print(\"latest_ok:\", len(latest_results))\n",
    "print(\"latest_errors:\", len(errors))\n",
    "print(\"elapsed_s:\", round(elapsed_s, 2))\n",
    "\n",
    "if errors:\n",
    "    print(\"first_error:\", errors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8dd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry rate-limited requests (HTTP 429) with backoff and Retry-After support.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"X-API-Key\": OPENAQ_API_KEY})\n",
    "\n",
    "def fetch_latest_with_retries(location_id: int, max_attempts: int = 5) -> dict:\n",
    "    url = f\"{OPENAQ_BASE_URL}/locations/{location_id}/latest\"\n",
    "    delay_s = 1.0\n",
    "\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        resp = session.get(url, timeout=30)\n",
    "\n",
    "        if resp.status_code == 429:\n",
    "            retry_after = resp.headers.get(\"Retry-After\")\n",
    "            if retry_after:\n",
    "                try:\n",
    "                    delay_s = max(delay_s, float(retry_after))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            time.sleep(delay_s)\n",
    "            delay_s = min(delay_s * 2, 30.0)\n",
    "            continue\n",
    "\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    raise RuntimeError(f\"Rate-limited after {max_attempts} attempts\")\n",
    "\n",
    "# Retry only the failures from the previous cell\n",
    "retry_ids = [loc_id for loc_id, _ in errors]\n",
    "\n",
    "retried_ok: list[dict] = []\n",
    "retried_errors: list[tuple[int, str]] = []\n",
    "\n",
    "t0 = time.time()\n",
    "for loc_id in retry_ids:\n",
    "    try:\n",
    "        retried_ok.append(fetch_latest_with_retries(loc_id))\n",
    "    except Exception as e:\n",
    "        retried_errors.append((loc_id, str(e)))\n",
    "\n",
    "elapsed_s = time.time() - t0\n",
    "\n",
    "latest_results.extend(retried_ok)\n",
    "errors = retried_errors\n",
    "\n",
    "print(\"retried_requested:\", len(retry_ids))\n",
    "print(\"retried_ok:\", len(retried_ok))\n",
    "print(\"remaining_errors:\", len(errors))\n",
    "print(\"retry_elapsed_s:\", round(elapsed_s, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize latest payloads and compute daily metrics.\n",
    "# This version works with the OpenAQ v3 /locations/{id}/latest schema we observed.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def parse_utc_datetime(dt_obj) -> datetime | None:\n",
    "    if not isinstance(dt_obj, dict):\n",
    "        return None\n",
    "    s = dt_obj.get(\"utc\")\n",
    "    if not isinstance(s, str) or not s:\n",
    "        return None\n",
    "    try:\n",
    "        if s.endswith(\"Z\"):\n",
    "            s = s[:-1] + \"+00:00\"\n",
    "        return datetime.fromisoformat(s).astimezone(timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "stale_cutoff = now_utc.timestamp() - (STALE_HOURS * 3600)\n",
    "\n",
    "rows: list[dict] = []\n",
    "missing_datetime = 0\n",
    "\n",
    "for payload in latest_results:\n",
    "    for m in payload.get(\"results\", []) or []:\n",
    "        dt = parse_utc_datetime(m.get(\"datetime\"))\n",
    "        if dt is None:\n",
    "            missing_datetime += 1\n",
    "        dt_ts = dt.timestamp() if dt else None\n",
    "\n",
    "        stale = (dt_ts is None) or (dt_ts < stale_cutoff)\n",
    "        coords = m.get(\"coordinates\") or {}\n",
    "        rows.append(\n",
    "            {\n",
    "                \"locationsId\": m.get(\"locationsId\"),\n",
    "                \"sensorsId\": m.get(\"sensorsId\"),\n",
    "                \"latitude\": coords.get(\"latitude\"),\n",
    "                \"longitude\": coords.get(\"longitude\"),\n",
    "                \"value\": m.get(\"value\"),\n",
    "                \"datetime_utc\": dt.isoformat().replace(\"+00:00\", \"Z\") if dt else None,\n",
    "                \"stale\": stale,\n",
    "            }\n",
    "        )\n",
    "\n",
    "total = len(rows)\n",
    "stale_count = sum(1 for r in rows if r[\"stale\"])\n",
    "stale_fraction = (stale_count / total) if total else 0.0\n",
    "\n",
    "# Useful \"top values\" view (across all sensors)\n",
    "numeric_rows = [r for r in rows if isinstance(r.get(\"value\"), (int, float))]\n",
    "top_values = sorted(numeric_rows, key=lambda r: r[\"value\"], reverse=True)[:5]\n",
    "\n",
    "print(\"locations_sampled:\", len(sample_ids))\n",
    "print(\"locations_with_latest:\", len(latest_results))\n",
    "print(\"measurements_total:\", total)\n",
    "print(\"missing_datetime:\", missing_datetime)\n",
    "print(\"stale_fraction:\", round(stale_fraction, 3))\n",
    "print(\"top_5_values:\", [(r[\"locationsId\"], r[\"value\"], r[\"datetime_utc\"]) for r in top_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68396ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich measurements with sensor metadata via /locations/{id}/sensors (throttled)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"X-API-Key\": OPENAQ_API_KEY})\n",
    "\n",
    "def get_with_backoff(url: str, *, max_attempts: int = 10, timeout_s: int = 30) -> requests.Response:\n",
    "    delay_s = 1.0\n",
    "    for _ in range(max_attempts):\n",
    "        r = session.get(url, timeout=timeout_s)\n",
    "\n",
    "        if r.status_code == 429:\n",
    "            retry_after = r.headers.get(\"Retry-After\")\n",
    "            if retry_after:\n",
    "                try:\n",
    "                    delay_s = max(delay_s, float(retry_after))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            time.sleep(delay_s)\n",
    "            delay_s = min(delay_s * 1.5, 30.0)\n",
    "            continue\n",
    "\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "\n",
    "    raise RuntimeError(f\"Rate-limited too long: {url}\")\n",
    "\n",
    "# Query sensors only for locations we actually sampled (NY-only).\n",
    "location_ids = sorted({loc[\"id\"] for loc in sampled_locations if \"id\" in loc})\n",
    "\n",
    "sensor_meta: dict[int, dict] = {}\n",
    "errors_sensors: list[tuple[int, str]] = []\n",
    "\n",
    "t0 = time.time()\n",
    "for i, loc_id in enumerate(location_ids, start=1):\n",
    "    url = f\"{OPENAQ_BASE_URL}/locations/{loc_id}/sensors\"\n",
    "    try:\n",
    "        payload = get_with_backoff(url).json()\n",
    "        for s in payload.get(\"results\", []) or []:\n",
    "            sid = s.get(\"id\")\n",
    "            param = s.get(\"parameter\") or {}\n",
    "            if isinstance(sid, int):\n",
    "                sensor_meta[sid] = {\n",
    "                    \"parameter_name\": param.get(\"name\"),\n",
    "                    \"units\": param.get(\"units\"),\n",
    "                }\n",
    "    except Exception as e:\n",
    "        errors_sensors.append((loc_id, str(e)))\n",
    "\n",
    "    # Gentle throttle to avoid sustained rate limiting.\n",
    "    time.sleep(0.25)\n",
    "\n",
    "elapsed_s = time.time() - t0\n",
    "\n",
    "missing_sensor_meta = 0\n",
    "for r in rows:\n",
    "    meta = sensor_meta.get(r.get(\"sensorsId\"))\n",
    "    if not meta:\n",
    "        missing_sensor_meta += 1\n",
    "        continue\n",
    "    r.update(meta)\n",
    "\n",
    "param_counts: dict[str, int] = {}\n",
    "for r in rows:\n",
    "    p = r.get(\"parameter_name\") or \"unknown\"\n",
    "    param_counts[p] = param_counts.get(p, 0) + 1\n",
    "\n",
    "top_params = sorted(param_counts.items(), key=lambda kv: (-kv[1], kv[0]))[:10]\n",
    "\n",
    "print(\"locations_queried_for_sensors:\", len(location_ids))\n",
    "print(\"unique_sensors_mapped:\", len(sensor_meta))\n",
    "print(\"rows_missing_sensor_meta:\", missing_sensor_meta)\n",
    "print(\"sensor_errors:\", len(errors_sensors))\n",
    "print(\"top_parameters:\", top_params)\n",
    "print(\"sensor_lookup_elapsed_s:\", round(elapsed_s, 2))\n",
    "\n",
    "if errors_sensors:\n",
    "    print(\"first_sensor_error:\", errors_sensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09db52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache sensor metadata to keep daily runs fast.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "CACHE_PATH = (Path(\"..\") / \"data\" / \"sensor_meta_cache.json\").resolve()\n",
    "CACHE_MAX_AGE_DAYS = 7\n",
    "\n",
    "def utc_now() -> datetime:\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "def load_cache(path: Path) -> dict[int, dict] | None:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        payload = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        saved_at = payload.get(\"saved_at_utc\")\n",
    "        if not isinstance(saved_at, str):\n",
    "            return None\n",
    "        saved_dt = datetime.fromisoformat(saved_at.replace(\"Z\", \"+00:00\"))\n",
    "        if utc_now() - saved_dt > timedelta(days=CACHE_MAX_AGE_DAYS):\n",
    "            return None\n",
    "        data = payload.get(\"sensor_meta\")\n",
    "        if not isinstance(data, dict):\n",
    "            return None\n",
    "        # keys come back as strings from JSON\n",
    "        return {int(k): v for k, v in data.items()}\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def save_cache(path: Path, data: dict[int, dict]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    payload = {\n",
    "        \"saved_at_utc\": utc_now().isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"sensor_meta\": {str(k): v for k, v in data.items()},\n",
    "    }\n",
    "    path.write_text(json.dumps(payload, indent=2, sort_keys=True), encoding=\"utf-8\")\n",
    "\n",
    "cached = load_cache(CACHE_PATH)\n",
    "if cached is None:\n",
    "    save_cache(CACHE_PATH, sensor_meta)\n",
    "    print(\"sensor cache: saved\", len(sensor_meta), \"entries to\", CACHE_PATH)\n",
    "else:\n",
    "    print(\"sensor cache: loaded\", len(cached), \"entries from\", CACHE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30359479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily metrics (parameter-aware) for notes + CSV\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "def parse_utc(s: str | None) -> datetime | None:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        if s.endswith(\"Z\"):\n",
    "            s = s[:-1] + \"+00:00\"\n",
    "        return datetime.fromisoformat(s).astimezone(timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "stale_cutoff = now_utc - timedelta(hours=STALE_HOURS)\n",
    "very_old_cutoff = now_utc - timedelta(days=30)\n",
    "\n",
    "total_rows = len(rows)\n",
    "stale_rows = 0\n",
    "very_old_rows = 0\n",
    "\n",
    "param_counts: dict[str, int] = {}\n",
    "param_stale_counts: dict[str, int] = {}\n",
    "param_units: dict[str, str] = {}\n",
    "\n",
    "latest_dt_seen: datetime | None = None\n",
    "oldest_dt_seen: datetime | None = None\n",
    "\n",
    "for r in rows:\n",
    "    p = r.get(\"parameter_name\") or \"unknown\"\n",
    "    param_counts[p] = param_counts.get(p, 0) + 1\n",
    "    if p not in param_units and r.get(\"units\"):\n",
    "        param_units[p] = str(r[\"units\"])\n",
    "\n",
    "    dt = parse_utc(r.get(\"datetime_utc\"))\n",
    "    if dt:\n",
    "        if latest_dt_seen is None or dt > latest_dt_seen:\n",
    "            latest_dt_seen = dt\n",
    "        if oldest_dt_seen is None or dt < oldest_dt_seen:\n",
    "            oldest_dt_seen = dt\n",
    "\n",
    "    is_stale = bool(r.get(\"stale\"))\n",
    "    if is_stale:\n",
    "        stale_rows += 1\n",
    "        param_stale_counts[p] = param_stale_counts.get(p, 0) + 1\n",
    "\n",
    "    if dt is None or dt < very_old_cutoff:\n",
    "        very_old_rows += 1\n",
    "\n",
    "stale_fraction = (stale_rows / total_rows) if total_rows else 0.0\n",
    "very_old_fraction = (very_old_rows / total_rows) if total_rows else 0.0\n",
    "\n",
    "top_params = sorted(param_counts.items(), key=lambda kv: (-kv[1], kv[0]))[:10]\n",
    "\n",
    "def top_values_for_parameter(param: str, n: int = 5) -> list[dict]:\n",
    "    candidates = [\n",
    "        r for r in rows\n",
    "        if (r.get(\"parameter_name\") or \"unknown\") == param and isinstance(r.get(\"value\"), (int, float))\n",
    "    ]\n",
    "    return sorted(candidates, key=lambda r: r[\"value\"], reverse=True)[:n]\n",
    "\n",
    "primary_param = top_params[0][0] if top_params else \"unknown\"\n",
    "primary_top = top_values_for_parameter(primary_param, n=5)\n",
    "\n",
    "print(\"locations_sampled:\", len(sample_ids))\n",
    "print(\"locations_with_latest:\", len(latest_results))\n",
    "print(\"measurements_total:\", total_rows)\n",
    "print(\"stale_fraction:\", round(stale_fraction, 3))\n",
    "print(\"very_old_fraction:\", round(very_old_fraction, 3))\n",
    "print(\"datetime_range_utc:\", (oldest_dt_seen, latest_dt_seen))\n",
    "print(\"top_parameters:\", top_params)\n",
    "print(\n",
    "    \"primary_param_top5:\",\n",
    "    [(r.get(\"locationsId\"), r.get(\"value\"), r.get(\"units\"), r.get(\"datetime_utc\")) for r in primary_top],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write today's artifacts in an idempotent way (safe to rerun).\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "NOTES_DIR = REPO_ROOT / \"notes\"\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NOTES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_csv_path = DATA_DIR / \"daily.csv\"\n",
    "note_path = NOTES_DIR / f\"{TODAY_UTC}.md\"\n",
    "\n",
    "\n",
    "def dt_to_z(dt: datetime | None) -> str:\n",
    "    if not dt:\n",
    "        return \"\"\n",
    "    return dt.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "\n",
    "csv_fields = [\n",
    "    \"date_utc\",\n",
    "    \"bbox\",\n",
    "    \"sample_size\",\n",
    "    \"locations_in_ny_boundary\",\n",
    "    \"locations_with_latest\",\n",
    "    \"measurements_total\",\n",
    "    \"api_latency_ms_locations\",\n",
    "    \"stale_hours\",\n",
    "    \"stale_fraction\",\n",
    "    \"very_old_fraction\",\n",
    "    \"oldest_datetime_utc\",\n",
    "    \"latest_datetime_utc\",\n",
    "    \"top_parameters\",\n",
    "]\n",
    "\n",
    "today_row = {\n",
    "    \"date_utc\": TODAY_UTC,\n",
    "    \"bbox\": BBOX,\n",
    "    \"sample_size\": len(sample_ids),\n",
    "    \"locations_in_ny_boundary\": len(ny_locations),\n",
    "    \"locations_with_latest\": len(latest_results),\n",
    "    \"measurements_total\": len(rows),\n",
    "    \"api_latency_ms_locations\": latency_ms,\n",
    "    \"stale_hours\": STALE_HOURS,\n",
    "    \"stale_fraction\": f\"{stale_fraction:.4f}\",\n",
    "    \"very_old_fraction\": f\"{very_old_fraction:.4f}\",\n",
    "    \"oldest_datetime_utc\": dt_to_z(oldest_dt_seen),\n",
    "    \"latest_datetime_utc\": dt_to_z(latest_dt_seen),\n",
    "    \"top_parameters\": \";\".join([f\"{p}:{c}\" for p, c in top_params]),\n",
    "}\n",
    "\n",
    "# Upsert by date_utc, and collapse any existing duplicates for the same date.\n",
    "existing: list[dict] = []\n",
    "if daily_csv_path.exists():\n",
    "    with daily_csv_path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            if r.get(\"date_utc\"):\n",
    "                existing.append(r)\n",
    "\n",
    "kept: list[dict] = [r for r in existing if r.get(\"date_utc\") != TODAY_UTC]\n",
    "kept.append(today_row)\n",
    "kept.sort(key=lambda r: r[\"date_utc\"])\n",
    "\n",
    "tmp_path = daily_csv_path.with_suffix(\".csv.tmp\")\n",
    "with tmp_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=csv_fields)\n",
    "    w.writeheader()\n",
    "    for r in kept:\n",
    "        w.writerow({k: r.get(k, \"\") for k in csv_fields})\n",
    "\n",
    "tmp_path.replace(daily_csv_path)\n",
    "\n",
    "# Daily note overwrites the same path for the day.\n",
    "lines: list[str] = []\n",
    "lines.append(f\"# NYS air quality daily summary — {TODAY_UTC} (UTC)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Technical summary\")\n",
    "lines.append(f\"- Locations in NY boundary: {len(ny_locations)}\")\n",
    "lines.append(f\"- Locations sampled: {len(sample_ids)}\")\n",
    "lines.append(f\"- Locations with latest data: {len(latest_results)}\")\n",
    "lines.append(f\"- Measurements normalized: {len(rows)}\")\n",
    "lines.append(f\"- API latency (locations catalog): {latency_ms} ms\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Data quality checks\")\n",
    "lines.append(f\"- Stale threshold: {STALE_HOURS} hours\")\n",
    "lines.append(f\"- Stale fraction: {stale_fraction:.3f}\")\n",
    "lines.append(f\"- Very old fraction (>30 days): {very_old_fraction:.3f}\")\n",
    "lines.append(f\"- Timestamp range (UTC): {dt_to_z(oldest_dt_seen)} → {dt_to_z(latest_dt_seen)}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Parameter coverage (top)\")\n",
    "for p, c in top_params:\n",
    "    unit = param_units.get(p, \"\")\n",
    "    unit_s = f\" ({unit})\" if unit else \"\"\n",
    "    lines.append(f\"- {p}{unit_s}: {c}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"### Highest values for {primary_param}\")\n",
    "if primary_top:\n",
    "    for r in primary_top:\n",
    "        loc_id = r.get(\"locationsId\")\n",
    "        val = r.get(\"value\")\n",
    "        unit = r.get(\"units\") or \"\"\n",
    "        dt = r.get(\"datetime_utc\") or \"\"\n",
    "        lines.append(f\"- location_id={loc_id}: {val} {unit} at {dt}\")\n",
    "else:\n",
    "    lines.append(\"- No numeric values available.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Plain-language summary\")\n",
    "lines.append(\n",
    "    \"This report tracks a stable set of monitoring locations within New York State and records the latest readings \"\n",
    "    \"available from OpenAQ. Some readings are old or missing; those are called out in the quality section so daily \"\n",
    "    \"changes are not misread as real shifts in air quality.\"\n",
    ")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"Today, the most commonly reported parameter in the sample was '{primary_param}'.\")\n",
    "note_path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Updated:\", daily_csv_path)\n",
    "print(\"Wrote:\", note_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVG chart: parameter coverage\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from xml.sax.saxutils import escape\n",
    "\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "report_dir = REPO_ROOT / \"reports\" / TODAY_UTC\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "chart_path = report_dir / \"parameter_coverage.svg\"\n",
    "\n",
    "labels = [p for p, _ in top_params]\n",
    "values = [c for _, c in top_params]\n",
    "\n",
    "width, height = 900, 360\n",
    "pad_left, pad_right, pad_top, pad_bottom = 60, 20, 30, 80\n",
    "plot_w = width - pad_left - pad_right\n",
    "plot_h = height - pad_top - pad_bottom\n",
    "\n",
    "max_v = max(values) if values else 1\n",
    "bar_w = plot_w / max(len(values), 1)\n",
    "\n",
    "def y_for(v: float) -> float:\n",
    "    return pad_top + (1 - (v / max_v)) * plot_h\n",
    "\n",
    "parts: list[str] = []\n",
    "parts.append(f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{width}\" height=\"{height}\">')\n",
    "parts.append(f'<rect x=\"0\" y=\"0\" width=\"{width}\" height=\"{height}\" fill=\"white\"/>')\n",
    "parts.append(f'<text x=\"{pad_left}\" y=\"20\" font-family=\"Arial, sans-serif\" font-size=\"16\">Parameter coverage (NY sample)</text>')\n",
    "\n",
    "# Axes\n",
    "parts.append(f'<line x1=\"{pad_left}\" y1=\"{pad_top}\" x2=\"{pad_left}\" y2=\"{pad_top + plot_h}\" stroke=\"black\"/>')\n",
    "parts.append(f'<line x1=\"{pad_left}\" y1=\"{pad_top + plot_h}\" x2=\"{pad_left + plot_w}\" y2=\"{pad_top + plot_h}\" stroke=\"black\"/>')\n",
    "\n",
    "for i, (lab, v) in enumerate(zip(labels, values)):\n",
    "    x = pad_left + i * bar_w + bar_w * 0.15\n",
    "    w = bar_w * 0.7\n",
    "    y = y_for(v)\n",
    "    h = (pad_top + plot_h) - y\n",
    "    parts.append(f'<rect x=\"{x:.2f}\" y=\"{y:.2f}\" width=\"{w:.2f}\" height=\"{h:.2f}\" fill=\"gray\"/>')\n",
    "\n",
    "    lx = pad_left + i * bar_w + bar_w * 0.5\n",
    "    ly = pad_top + plot_h + 10\n",
    "    parts.append(\n",
    "        f'<text x=\"{lx:.2f}\" y=\"{ly:.2f}\" font-family=\"Arial, sans-serif\" font-size=\"11\" '\n",
    "        f'transform=\"rotate(45 {lx:.2f} {ly:.2f})\" text-anchor=\"start\">{escape(lab)}</text>'\n",
    "    )\n",
    "\n",
    "parts.append(\n",
    "    f'<text x=\"15\" y=\"{pad_top + plot_h/2:.2f}\" font-family=\"Arial, sans-serif\" font-size=\"12\" '\n",
    "    f'transform=\"rotate(-90 15 {pad_top + plot_h/2:.2f})\" text-anchor=\"middle\">measurement count</text>'\n",
    ")\n",
    "\n",
    "parts.append(\"</svg>\")\n",
    "\n",
    "chart_path.write_text(\"\\n\".join(parts) + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"Wrote:\", chart_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fa590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVG map: NY outline + sample points, colored by primary parameter, radius scaled by value\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from xml.sax.saxutils import escape\n",
    "\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "report_dir = REPO_ROOT / \"reports\" / TODAY_UTC\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "map_path = report_dir / \"map.svg\"\n",
    "\n",
    "primary = primary_param\n",
    "unit = param_units.get(primary, \"\")\n",
    "\n",
    "USE_ONLY_FRESH_FOR_SCALE = True\n",
    "\n",
    "def pick_latest_value_by_location(parameter: str) -> dict[int, tuple[float, str, bool]]:\n",
    "    out: dict[int, tuple[float, str, bool]] = {}\n",
    "    for r in rows:\n",
    "        if (r.get(\"parameter_name\") or \"unknown\") != parameter:\n",
    "            continue\n",
    "        loc_id = r.get(\"locationsId\")\n",
    "        val = r.get(\"value\")\n",
    "        dt = r.get(\"datetime_utc\") or \"\"\n",
    "        stale = bool(r.get(\"stale\"))\n",
    "        if not isinstance(loc_id, int) or not isinstance(val, (int, float)):\n",
    "            continue\n",
    "        prev = out.get(loc_id)\n",
    "        if prev is None or dt > prev[1]:\n",
    "            out[loc_id] = (float(val), dt, stale)\n",
    "    return out\n",
    "\n",
    "loc_value = pick_latest_value_by_location(primary)\n",
    "\n",
    "points: list[tuple[float, float, float | None, bool]] = []\n",
    "for loc in ny_locations:\n",
    "    loc_id = loc.get(\"id\")\n",
    "    c = loc.get(\"coordinates\") or {}\n",
    "    try:\n",
    "        lon = float(c.get(\"longitude\"))\n",
    "        lat = float(c.get(\"latitude\"))\n",
    "    except (TypeError, ValueError):\n",
    "        continue\n",
    "    v = loc_value.get(loc_id) if isinstance(loc_id, int) else None\n",
    "    if v:\n",
    "        points.append((lon, lat, v[0], v[2]))\n",
    "    else:\n",
    "        points.append((lon, lat, None, True))\n",
    "\n",
    "def percentile(values: list[float], p: float) -> float:\n",
    "    if not values:\n",
    "        return 0.0\n",
    "    xs = sorted(values)\n",
    "    k = (len(xs) - 1) * p\n",
    "    f = int(k)\n",
    "    c = min(f + 1, len(xs) - 1)\n",
    "    if c == f:\n",
    "        return xs[f]\n",
    "    return xs[f] + (xs[c] - xs[f]) * (k - f)\n",
    "\n",
    "def clamp01(x: float) -> float:\n",
    "    return max(0.0, min(1.0, x))\n",
    "\n",
    "def lerp(a: float, b: float, t: float) -> float:\n",
    "    return a + (b - a) * t\n",
    "\n",
    "def color_ramp(t: float) -> str:\n",
    "    t = clamp01(t)\n",
    "    if t < 0.5:\n",
    "        tt = t / 0.5\n",
    "        r = int(lerp(52, 243, tt))\n",
    "        g = int(lerp(152, 156, tt))\n",
    "        b = int(lerp(219, 18, tt))\n",
    "    else:\n",
    "        tt = (t - 0.5) / 0.5\n",
    "        r = int(lerp(243, 231, tt))\n",
    "        g = int(lerp(156, 76, tt))\n",
    "        b = int(lerp(18, 60, tt))\n",
    "    return f\"rgb({r},{g},{b})\"\n",
    "\n",
    "vals_all = [v for _, _, v, _ in points if isinstance(v, (int, float))]\n",
    "vals_fresh = [v for _, _, v, stale in points if isinstance(v, (int, float)) and (not stale)]\n",
    "scale_vals = vals_fresh if (USE_ONLY_FRESH_FOR_SCALE and vals_fresh) else vals_all\n",
    "\n",
    "vmin = percentile(scale_vals, 0.05)\n",
    "vmax = percentile(scale_vals, 0.95)\n",
    "if vmax <= vmin:\n",
    "    vmax = vmin + 1.0\n",
    "\n",
    "def value_to_t(v: float) -> float:\n",
    "    return clamp01((v - vmin) / (vmax - vmin))\n",
    "\n",
    "def color_for(v: float | None, stale: bool) -> str:\n",
    "    if v is None:\n",
    "        return \"lightgray\"\n",
    "    if stale:\n",
    "        return \"rgb(180,180,180)\"\n",
    "    return color_ramp(value_to_t(v))\n",
    "\n",
    "def radius_for(v: float | None, stale: bool) -> float:\n",
    "    if v is None:\n",
    "        return 3.5\n",
    "    if stale:\n",
    "        return 3.5\n",
    "    t = value_to_t(v)\n",
    "    return 3.0 + t * 6.0  # 3 -> 9\n",
    "\n",
    "# NY polygon bounds for projection\n",
    "geom = ny_feature[\"geometry\"]\n",
    "coords = geom[\"coordinates\"]\n",
    "\n",
    "lons, lats = [], []\n",
    "for poly in coords:\n",
    "    for lon, lat in poly[0]:\n",
    "        lons.append(float(lon))\n",
    "        lats.append(float(lat))\n",
    "\n",
    "min_lon, max_lon = min(lons), max(lons)\n",
    "min_lat, max_lat = min(lats), max(lats)\n",
    "\n",
    "width, height = 900, 520\n",
    "pad = 20\n",
    "\n",
    "def project(lon: float, lat: float) -> tuple[float, float]:\n",
    "    x = pad + (lon - min_lon) / (max_lon - min_lon) * (width - 2 * pad)\n",
    "    y = pad + (max_lat - lat) / (max_lat - min_lat) * (height - 2 * pad)\n",
    "    return x, y\n",
    "\n",
    "parts: list[str] = []\n",
    "parts.append(f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{width}\" height=\"{height}\">')\n",
    "parts.append(f'<rect x=\"0\" y=\"0\" width=\"{width}\" height=\"{height}\" fill=\"white\"/>')\n",
    "\n",
    "title = f\"NY sample map — {primary}\"\n",
    "parts.append(f'<text x=\"{pad}\" y=\"18\" font-family=\"Arial, sans-serif\" font-size=\"16\">{escape(title)}</text>')\n",
    "\n",
    "# Outline\n",
    "for poly in coords:\n",
    "    outer = poly[0]\n",
    "    d = []\n",
    "    for i, (lon, lat) in enumerate(outer):\n",
    "        x, y = project(float(lon), float(lat))\n",
    "        d.append((\"M\" if i == 0 else \"L\") + f\"{x:.2f},{y:.2f}\")\n",
    "    d.append(\"Z\")\n",
    "    parts.append(f'<path d=\"{\" \".join(d)}\" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/>')\n",
    "\n",
    "# Points\n",
    "for lon, lat, v, stale in points:\n",
    "    x, y = project(lon, lat)\n",
    "    fill = color_for(v, stale)\n",
    "    r = radius_for(v, stale)\n",
    "    parts.append(f'<circle cx=\"{x:.2f}\" cy=\"{y:.2f}\" r=\"{r:.2f}\" fill=\"{fill}\" stroke=\"black\" stroke-width=\"0.6\"/>')\n",
    "\n",
    "# Legend\n",
    "legend_y = height - 44\n",
    "legend_x = pad\n",
    "sw = 18\n",
    "\n",
    "parts.append(f'<text x=\"{legend_x}\" y=\"{legend_y}\" font-family=\"Arial, sans-serif\" font-size=\"12\">Scale (p5–p95):</text>')\n",
    "for i, (label, t) in enumerate([(\"low\", 0.0), (\"mid\", 0.5), (\"high\", 1.0)]):\n",
    "    x = legend_x + 100 + i * 120\n",
    "    parts.append(f'<rect x=\"{x}\" y=\"{legend_y - 12}\" width=\"{sw}\" height=\"{sw}\" fill=\"{color_ramp(t)}\" stroke=\"black\" stroke-width=\"0.6\"/>')\n",
    "    parts.append(f'<text x=\"{x + 26}\" y=\"{legend_y + 2}\" font-family=\"Arial, sans-serif\" font-size=\"12\">{escape(label)}</text>')\n",
    "\n",
    "parts.append(\n",
    "    f'<text x=\"{pad}\" y=\"{height - 18}\" font-family=\"Arial, sans-serif\" font-size=\"12\">'\n",
    "    f\"p5={vmin:.2f}, p95={vmax:.2f} {escape(unit)}; stale points shown in gray\"\n",
    "    f\"</text>\"\n",
    ")\n",
    "\n",
    "parts.append(\"</svg>\")\n",
    "\n",
    "map_path.write_text(\"\\n\".join(parts) + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"Wrote:\", map_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily AQ notebook — baseline config + sanity checks.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Assumption: this notebook lives in ./notebooks, so repo root is one directory up.\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "NOTES_DIR = REPO_ROOT / \"notes\"\n",
    "REPORTS_DIR = REPO_ROOT / \"reports\"\n",
    "\n",
    "# Required secret: expected to be provided via your environment (not committed to git).\n",
    "API_KEY_ENV = \"OPENAQ_API_KEY\"\n",
    "OPENAQ_API_KEY = os.getenv(API_KEY_ENV)\n",
    "if not OPENAQ_API_KEY:\n",
    "    raise RuntimeError(f\"Missing environment variable: {API_KEY_ENV}\")\n",
    "\n",
    "# NYS rough bounding box (minLon,minLat,maxLon,maxLat). Adjust later if you want tighter bounds.\n",
    "BBOX = os.getenv(\"BBOX\", \"-79.8,40.4,-71.6,45.1\")\n",
    "\n",
    "# Stable sample size (locations). Keep modest to avoid slow daily runs.\n",
    "SAMPLE_SIZE = int(os.getenv(\"SAMPLE_SIZE\", \"100\"))\n",
    "\n",
    "# Data quality: consider readings older than this \"stale\".\n",
    "STALE_HOURS = int(os.getenv(\"STALE_HOURS\", \"12\"))\n",
    "\n",
    "# Today's date (UTC) is used for file paths.\n",
    "TODAY_UTC = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"BBOX:\", BBOX)\n",
    "print(\"SAMPLE_SIZE:\", SAMPLE_SIZE)\n",
    "print(\"STALE_HOURS:\", STALE_HOURS)\n",
    "print(\"TODAY_UTC:\", TODAY_UTC)\n",
    "print(\"API key loaded:\", \"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nys_aq.daily import run_daily\n",
    "\n",
    "cfg = run_daily()\n",
    "cfg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nys-aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
