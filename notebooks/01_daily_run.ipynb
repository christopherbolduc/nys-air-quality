{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45248eae",
   "metadata": {},
   "source": [
    "# NYS Air Quality — Exploratory Notebook\n",
    "\n",
    "This notebook provides a compact, reproducible walkthrough of the NYS daily air-quality pipeline using OpenAQ data.\n",
    "\n",
    "**What this notebook is**\n",
    "- A developer-facing companion to the production pipeline.\n",
    "- A convenient way to run a single report date locally and preview the generated outputs (note, map, coverage chart).\n",
    "\n",
    "**What this notebook is not**\n",
    "- The production job runner. Scheduled execution is handled by GitHub Actions calling `scripts/run_daily.py`.\n",
    "\n",
    "**Notes**\n",
    "- The pipeline is idempotent per report date: reruns overwrite that day’s note and report artifacts and upsert the corresponding CSV row.\n",
    "- Repository outputs are stored in `notes/` and `reports/`. Notebook cell outputs should be cleared before commits to keep diffs small and reviewable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily AQ notebook — configuration\n",
    "# Notebook purpose: exploratory analysis of the daily pipeline outputs.\n",
    "# Source of truth for production runs: scripts/run_daily.py + src/nys_aq/.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "REPO_ROOT = Path.cwd().parent  # notebooks/ -> repo root\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "NOTES_DIR = REPO_ROOT / \"notes\"\n",
    "REPORTS_DIR = REPO_ROOT / \"reports\"\n",
    "\n",
    "ENV_PATH = REPO_ROOT / \".env\"\n",
    "BOUNDARY_PATH = DATA_DIR / \"nys_boundary.geojson\"\n",
    "\n",
    "OPENAQ_BASE_URL = \"https://api.openaq.org/v3\"\n",
    "BBOX = os.getenv(\"BBOX\", \"-79.8,40.45,-71.85,45.1\")\n",
    "SAMPLE_SIZE = int(os.getenv(\"SAMPLE_SIZE\", \"100\"))\n",
    "STALE_HOURS = int(os.getenv(\"STALE_HOURS\", \"12\"))\n",
    "\n",
    "def load_env_file(path: Path) -> None:\n",
    "    \"\"\"Load KEY=VALUE pairs from a .env file into the process environment (no overwrite).\"\"\"\n",
    "    if not path.exists():\n",
    "        return\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        s = line.strip()\n",
    "        if not s or s.startswith(\"#\") or \"=\" not in s:\n",
    "            continue\n",
    "        k, v = s.split(\"=\", 1)\n",
    "        os.environ.setdefault(k.strip(), v.strip().strip('\"').strip(\"'\"))\n",
    "\n",
    "load_env_file(ENV_PATH)\n",
    "\n",
    "OPENAQ_API_KEY = os.getenv(\"OPENAQ_API_KEY\")\n",
    "if not OPENAQ_API_KEY:\n",
    "    raise RuntimeError(\"Missing OPENAQ_API_KEY. Add it to .env or export it in your shell.\")\n",
    "\n",
    "print(\"repo_root:\", REPO_ROOT)\n",
    "print(\"bbox:\", BBOX)\n",
    "print(\"sample_size:\", SAMPLE_SIZE)\n",
    "print(\"stale_hours:\", STALE_HOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch OpenAQ location catalog (bbox is a performance hint; NY membership is enforced by GeoJSON)\n",
    "\n",
    "t0 = time.time()\n",
    "resp = requests.get(\n",
    "    f\"{OPENAQ_BASE_URL}/locations\",\n",
    "    params={\"bbox\": BBOX, \"iso\": \"US\", \"limit\": 1000},\n",
    "    headers={\"X-API-Key\": OPENAQ_API_KEY},\n",
    "    timeout=30,\n",
    ")\n",
    "latency_ms = int((time.time() - t0) * 1000)\n",
    "\n",
    "resp.raise_for_status()\n",
    "payload = resp.json()\n",
    "\n",
    "locations_found = payload.get(\"meta\", {}).get(\"found\")\n",
    "locations = payload.get(\"results\", []) or []\n",
    "\n",
    "print(\"status:\", resp.status_code)\n",
    "print(\"latency_ms:\", latency_ms)\n",
    "print(\"locations_found:\", locations_found)\n",
    "print(\"locations_returned:\", len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16638e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYS boundary (GeoJSON Feature)\n",
    "\n",
    "if not BOUNDARY_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing boundary file: {BOUNDARY_PATH}\")\n",
    "\n",
    "ny_feature = json.loads(BOUNDARY_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "geom = ny_feature.get(\"geometry\") or {}\n",
    "geom_type = geom.get(\"type\")\n",
    "coords = geom.get(\"coordinates\")\n",
    "\n",
    "if geom_type not in {\"Polygon\", \"MultiPolygon\"}:\n",
    "    raise ValueError(f\"Unsupported geometry type: {geom_type}\")\n",
    "\n",
    "print(\"boundary_geometry:\", geom_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter locations to points inside the NYS boundary\n",
    "\n",
    "def point_in_ring(lon: float, lat: float, ring: list[list[float]]) -> bool:\n",
    "    \"\"\"Ray casting point-in-polygon for a single ring.\"\"\"\n",
    "    inside = False\n",
    "    j = len(ring) - 1\n",
    "    for i in range(len(ring)):\n",
    "        xi, yi = ring[i]\n",
    "        xj, yj = ring[j]\n",
    "        intersects = ((yi > lat) != (yj > lat)) and (\n",
    "            lon < (xj - xi) * (lat - yi) / ((yj - yi) or 1e-15) + xi\n",
    "        )\n",
    "        if intersects:\n",
    "            inside = not inside\n",
    "        j = i\n",
    "    return inside\n",
    "\n",
    "def point_in_polygon(lon: float, lat: float, polygon: list[list[list[float]]]) -> bool:\n",
    "    \"\"\"polygon: [outer_ring, hole1, hole2, ...]\"\"\"\n",
    "    if not polygon:\n",
    "        return False\n",
    "    if not point_in_ring(lon, lat, polygon[0]):\n",
    "        return False\n",
    "    for hole in polygon[1:]:\n",
    "        if point_in_ring(lon, lat, hole):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def point_in_multipolygon(lon: float, lat: float, multipolygon: list) -> bool:\n",
    "    for polygon in multipolygon:\n",
    "        if point_in_polygon(lon, lat, polygon):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def loc_lon_lat(loc: dict) -> tuple[float | None, float | None]:\n",
    "    c = loc.get(\"coordinates\") or {}\n",
    "    try:\n",
    "        return float(c.get(\"longitude\")), float(c.get(\"latitude\"))\n",
    "    except (TypeError, ValueError):\n",
    "        return None, None\n",
    "\n",
    "geom = ny_feature[\"geometry\"]\n",
    "geom_type = geom[\"type\"]\n",
    "coords = geom[\"coordinates\"]\n",
    "\n",
    "ny_locations: list[dict] = []\n",
    "missing_coords = 0\n",
    "\n",
    "for loc in locations:\n",
    "    lon, lat = loc_lon_lat(loc)\n",
    "    if lon is None or lat is None:\n",
    "        missing_coords += 1\n",
    "        continue\n",
    "\n",
    "    inside = point_in_polygon(lon, lat, coords) if geom_type == \"Polygon\" else point_in_multipolygon(lon, lat, coords)\n",
    "    if inside:\n",
    "        ny_locations.append(loc)\n",
    "\n",
    "print(\"locations_total:\", len(locations))\n",
    "print(\"missing_coords:\", missing_coords)\n",
    "print(\"ny_locations:\", len(ny_locations))\n",
    "print(\"first_5_names:\", [l.get(\"name\") for l in ny_locations[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8dd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable sample of NY locations (deterministic ordering by location id + bbox)\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def stable_rank(location_id: int, bbox: str) -> int:\n",
    "    raw = f\"{location_id}|{bbox}\".encode(\"utf-8\")\n",
    "    return int(hashlib.sha256(raw).hexdigest(), 16)\n",
    "\n",
    "ny_locations_with_id = [l for l in ny_locations if isinstance(l.get(\"id\"), int)]\n",
    "ny_sorted = sorted(ny_locations_with_id, key=lambda l: stable_rank(l[\"id\"], BBOX))\n",
    "\n",
    "sampled_locations = ny_sorted[: min(SAMPLE_SIZE, len(ny_sorted))]\n",
    "sample_ids = [l[\"id\"] for l in sampled_locations]\n",
    "sample_label = \"All NY locations (with coordinates)\" if len(sample_ids) == len(ny_sorted) else f\"Stable sample (n={len(sample_ids)})\"\n",
    "\n",
    "print(\"ny_locations_with_id:\", len(ny_sorted))\n",
    "print(\"sample_size_actual:\", len(sample_ids))\n",
    "print(\"sample_label:\", sample_label)\n",
    "print(\"sample_first5_ids:\", sample_ids[:5])\n",
    "print(\"sample_first5_names:\", [l.get(\"name\") for l in sampled_locations[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch latest measurements with retry/backoff (handles HTTP 429) and normalize to rows\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"X-API-Key\": OPENAQ_API_KEY})\n",
    "\n",
    "def fetch_latest(location_id: int, *, max_attempts: int = 6, timeout_s: int = 30) -> dict:\n",
    "    url = f\"{OPENAQ_BASE_URL}/locations/{location_id}/latest\"\n",
    "    delay_s = 1.0\n",
    "\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        resp = session.get(url, timeout=timeout_s)\n",
    "\n",
    "        if resp.status_code == 429:\n",
    "            retry_after = resp.headers.get(\"Retry-After\")\n",
    "            if retry_after:\n",
    "                try:\n",
    "                    delay_s = max(delay_s, float(retry_after))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            time.sleep(delay_s)\n",
    "            delay_s = min(delay_s * 2, 30.0)\n",
    "            continue\n",
    "\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    raise RuntimeError(f\"Rate-limited too long for location_id={location_id}\")\n",
    "\n",
    "def parse_utc_datetime(dt_obj: object) -> datetime | None:\n",
    "    if not isinstance(dt_obj, dict):\n",
    "        return None\n",
    "    s = dt_obj.get(\"utc\")\n",
    "    if not isinstance(s, str) or not s:\n",
    "        return None\n",
    "    try:\n",
    "        if s.endswith(\"Z\"):\n",
    "            s = s[:-1] + \"+00:00\"\n",
    "        return datetime.fromisoformat(s).astimezone(timezone.utc)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "latest_payloads: list[dict] = []\n",
    "errors: list[tuple[int, str]] = []\n",
    "\n",
    "for loc_id in sample_ids:\n",
    "    try:\n",
    "        latest_payloads.append(fetch_latest(loc_id))\n",
    "    except Exception as e:\n",
    "        errors.append((loc_id, f\"{type(e).__name__}: {e}\"))\n",
    "\n",
    "elapsed_s = time.time() - t0\n",
    "\n",
    "print(\"locations_requested:\", len(sample_ids))\n",
    "print(\"latest_ok:\", len(latest_payloads))\n",
    "print(\"latest_errors:\", len(errors))\n",
    "print(\"latest_elapsed_s:\", round(elapsed_s, 2))\n",
    "if errors:\n",
    "    print(\"first_error:\", errors[0])\n",
    "\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "stale_cutoff = now_utc.timestamp() - (STALE_HOURS * 3600)\n",
    "\n",
    "rows: list[dict] = []\n",
    "missing_datetime = 0\n",
    "\n",
    "for payload in latest_payloads:\n",
    "    for m in (payload.get(\"results\") or []):\n",
    "        dt = parse_utc_datetime(m.get(\"datetime\"))\n",
    "        if dt is None:\n",
    "            missing_datetime += 1\n",
    "        dt_ts = dt.timestamp() if dt else None\n",
    "\n",
    "        stale = (dt_ts is None) or (dt_ts < stale_cutoff)\n",
    "        coords = m.get(\"coordinates\") or {}\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"locationsId\": m.get(\"locationsId\"),\n",
    "                \"sensorsId\": m.get(\"sensorsId\"),\n",
    "                \"latitude\": coords.get(\"latitude\"),\n",
    "                \"longitude\": coords.get(\"longitude\"),\n",
    "                \"value\": m.get(\"value\"),\n",
    "                \"datetime_utc\": dt.isoformat().replace(\"+00:00\", \"Z\") if dt else None,\n",
    "                \"stale\": stale,\n",
    "            }\n",
    "        )\n",
    "\n",
    "total = len(rows)\n",
    "stale_count = sum(1 for r in rows if r[\"stale\"])\n",
    "stale_fraction = (stale_count / total) if total else 0.0\n",
    "\n",
    "numeric_rows = [r for r in rows if isinstance(r.get(\"value\"), (int, float))]\n",
    "top_values = sorted(numeric_rows, key=lambda r: float(r[\"value\"]), reverse=True)[:5]\n",
    "\n",
    "print(\"locations_sampled:\", len(sample_ids))\n",
    "print(\"measurements_total:\", total)\n",
    "print(\"missing_datetime:\", missing_datetime)\n",
    "print(\"stale_fraction:\", round(stale_fraction, 3))\n",
    "print(\"top_5_values:\", [(r.get(\"locationsId\"), r.get(\"value\"), r.get(\"datetime_utc\")) for r in top_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30359479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the production pipeline for a chosen report date (yesterday ET by default)\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from nys_aq.daily import run_daily\n",
    "\n",
    "# Optional: set a specific date for reproducibility\n",
    "# report_date = date.fromisoformat(\"2026-02-01\")\n",
    "report_date = None\n",
    "\n",
    "cfg = run_daily(run_date=report_date)\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the generated outputs (note + CSV row)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "report_date = cfg.run_date.isoformat()\n",
    "\n",
    "note_path = cfg.repo_root / \"notes\" / f\"{report_date}.md\"\n",
    "csv_path = cfg.repo_root / \"data\" / \"daily.csv\"\n",
    "report_dir = cfg.repo_root / \"reports\" / report_date\n",
    "\n",
    "print(\"note:\", note_path)\n",
    "print(\"report_dir:\", report_dir)\n",
    "\n",
    "print(\"\\n--- note (top) ---\\n\")\n",
    "print(\"\\n\".join(note_path.read_text(encoding=\"utf-8\").splitlines()[:25]))\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "row = df.loc[df[\"report_date\"] == report_date]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs (generated by the pipeline) — quick links for review\n",
    "\n",
    "from IPython.display import SVG, Markdown, display\n",
    "\n",
    "report_date = cfg.run_date.isoformat()\n",
    "note_path = cfg.repo_root / \"notes\" / f\"{report_date}.md\"\n",
    "report_dir = cfg.repo_root / \"reports\" / report_date\n",
    "\n",
    "display(Markdown(f\"## Outputs for `{report_date}`\"))\n",
    "display(Markdown(f\"- Note: `{note_path.relative_to(cfg.repo_root)}`\"))\n",
    "display(Markdown(f\"- Map: `{(report_dir / 'map.svg').relative_to(cfg.repo_root)}`\"))\n",
    "display(Markdown(f\"- Coverage: `{(report_dir / 'parameter_coverage.svg').relative_to(cfg.repo_root)}`\"))\n",
    "\n",
    "display(Markdown(\"### Note preview\"))\n",
    "display(Markdown(\"\\n\".join(note_path.read_text(encoding='utf-8').splitlines()[:40])))\n",
    "\n",
    "display(Markdown(\"### Parameter coverage\"))\n",
    "display(SVG(filename=str(report_dir / \"parameter_coverage.svg\")))\n",
    "\n",
    "display(Markdown(\"### Map\"))\n",
    "display(SVG(filename=str(report_dir / \"map.svg\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nys-aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
